{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv, datetime, time, json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tensorflow import keras\n",
    "from zipfile import ZipFile\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras import Sequential\n",
    "from keras.layers import Input, TimeDistributed, Dense, Lambda, concatenate, Dropout, BatchNormalization, LSTM, Concatenate, Embedding, Bidirectional\n",
    "from keras.utils import to_categorical\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 315\n",
    "NUM_EPOCHS = 10 \n",
    "BATCH_SIZE = 32\n",
    "DROPOUT = 0.1\n",
    "TEST_PORTION = 0.1\n",
    "VALIDATION = 0.1\n",
    "LSTM_DIM = 64\n",
    "LSTM_REGULARIZATION = 0.0001\n",
    "LSTM_DROPOUT = 0.1\n",
    "\n",
    "MAX_N_WORDS = 30000\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LEN = 1000\n",
    "\n",
    "OPTIMIZER = 'adam'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vincent/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/vincent/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vincent/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/vincent/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/vincent/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "nltk_stopwords.remove('no')\n",
    "nltk_stopwords.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Processing\n",
    "porter_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "punctuation_translator = str.maketrans('', '', string.punctuation)\n",
    "def preprocess_text(s, lowercase=True, remove_stopwords=True, remove_punctuation=True, stemmer=None, lemmatizer=None):\n",
    "    tokens = word_tokenize(s)\n",
    "\n",
    "    if lemmatizer is not None:\n",
    "        tokens = lemmatize_tokens(lemmatizer, tokens)\n",
    "    elif stemmer is not None:\n",
    "        tokens = stem_tokens(stemmer, tokens)\n",
    "\n",
    "    if lowercase:\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        tokens = [token for token in tokens if not token in nltk_stopwords]\n",
    "    \n",
    "    # Remove all punctuation marks if needed (note: also converts, e.g, \"Mr.\" to \"Mr\")\n",
    "    if remove_punctuation:\n",
    "        tokens = [ ''.join(c for c in s if c not in string.punctuation) for s in tokens ]\n",
    "        tokens = [ token for token in tokens if len(token) > 0 ] # Remove \"empty\" tokens\n",
    "\n",
    "    # if (len(tokens) == 0):\n",
    "    #     print('len = 0: '+ s)\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def remove_punctuation(s):\n",
    "    return s.translate(punctuation_translator)\n",
    "\n",
    "def lemmatize_tokens(lemmatizer, tokens):\n",
    "    pos_tag_list = pos_tag(tokens)\n",
    "    for idx, (token, tag) in enumerate(pos_tag_list):\n",
    "        tag_simple = tag[0].lower() # Converts, e.g., \"VBD\" to \"c\"\n",
    "        if tag_simple in ['n', 'v', 'j']:\n",
    "            word_type = tag_simple.replace('j', 'a') \n",
    "        else:\n",
    "            word_type = 'n'\n",
    "        lemmatized_token = lemmatizer.lemmatize(token, pos=word_type)\n",
    "        tokens[idx] = lemmatized_token\n",
    "    return tokens\n",
    "\n",
    "def stem_tokens(stemmer, tokens):\n",
    "    for idx, token in enumerate(tokens):\n",
    "        tokens[idx] = stemmer.stem(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_q/prj61rbn34xftt0_fw1w47sw0000gn/T/ipykernel_52766/3678302757.py:24: FutureWarning: In a future version of pandas all arguments of Series.dropna will be keyword-only.\n",
      "  content = data.iloc[:,1].dropna(\"\")\n"
     ]
    }
   ],
   "source": [
    "train_file = 'Data/twitter_MBTI.csv'\n",
    "labels = {\n",
    "          'infp':0,\n",
    "          'infj':1,\n",
    "          'intp':2,\n",
    "          'intj':3,\n",
    "          'istp':4,\n",
    "          'istj':5,\n",
    "          'isfj':6,\n",
    "          'isfp':7,\n",
    "          'enfp':8,\n",
    "          'entp':9,\n",
    "          'enfj':10,\n",
    "          'entj':11,\n",
    "          'estp':12,\n",
    "          'estj':13,\n",
    "          'esfj':14,\n",
    "          'esfp':15,\n",
    "          }\n",
    "\n",
    "with open(train_file, \"r\") as f:\n",
    "    data = f.readlines()\n",
    "data = pd.read_csv(train_file)\n",
    "content = data.iloc[:,1].dropna(\"\")\n",
    "\n",
    "# X = [ preprocess_text(x, remove_stopwords=True, remove_punctuation=True, lemmatizer=None) for x in content ]\n",
    "# Y = data.iloc[:,2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_and_valid_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "valid_data, test_data = train_test_split(test_and_valid_data, test_size=0.5, random_state=42)\n",
    "\n",
    "content = train_data.iloc[:,1].dropna()\n",
    "X_train = [ preprocess_text(x, remove_stopwords=True, remove_punctuation=True, lemmatizer=None) for x in content ]\n",
    "Y_train = train_data.iloc[:,2]\n",
    "\n",
    "test_content = test_data.iloc[:,1].dropna()\n",
    "X_test = [ preprocess_text(x, remove_stopwords=True, remove_punctuation=True, lemmatizer=None) for x in test_content ]\n",
    "Y_test = train_data.iloc[:,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(X_train)\n",
    "# print(X_df.tail())\n",
    "Y_df = pd.DataFrame(Y_train)\n",
    "\n",
    "X_df.reset_index(drop=True, inplace=True)\n",
    "# print(X_df.head())\n",
    "Y_df.reset_index(drop=True, inplace=True)\n",
    "data_df = pd.concat([X_df, Y_df], axis = 1)\n",
    "data_df.to_csv(\"Data/preprocessed_train.csv\", index = False)\n",
    "data_saved = pd.read_csv(\"Data/preprocessed_train.csv\")\n",
    "X_df = data_saved.iloc[:,0].fillna(\"\").tolist()\n",
    "Y_df = data_saved.iloc[:,1].fillna(\"\").tolist()\n",
    "X_train = X_df\n",
    "y_train = [ labels[y] for y in Y_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(X_test)\n",
    "# print(X_df.tail())\n",
    "Y_df = pd.DataFrame(Y_test)\n",
    "\n",
    "X_df.reset_index(drop=True, inplace=True)\n",
    "# print(X_df.head())\n",
    "Y_df.reset_index(drop=True, inplace=True)\n",
    "data_df = pd.concat([X_df, Y_df], axis = 1)\n",
    "data_df.to_csv(\"Data/preprocessed_test.csv\", index = False)\n",
    "data_saved = pd.read_csv(\"Data/preprocessed_test.csv\")\n",
    "X_df = data_saved.iloc[:,0].fillna(\"\").tolist()\n",
    "Y_df = data_saved.iloc[:,1].fillna(\"\").tolist()\n",
    "X_test = X_df\n",
    "y_test = [ labels[y] for y in Y_df]\n",
    "y_train = to_categorical(y_train, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words from training corpus: 902634\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_N_WORDS)\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "#Training Set\n",
    "train_idx_seq = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "#Testing Set\n",
    "test_idx_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "word_idx = tokenizer.word_index\n",
    "print(\"Total words from training corpus: %d\" % len(word_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_q/prj61rbn34xftt0_fw1w47sw0000gn/T/ipykernel_52766/1113789756.py:8: FutureWarning: Using short name for 'orient' is deprecated. Only the options: ('dict', list, 'series', 'split', 'records', 'index') will be used in a future version. Use one of the above to silence this warning.\n",
      "  word_idx_saved = pd.read_csv(\"Data/word2idx.csv\").to_dict('r')[0]\n"
     ]
    }
   ],
   "source": [
    "# Saving word to idx dictionary\n",
    "word2idx = pd.DataFrame([word_idx])\n",
    "\n",
    "#save dataframe to csv file\n",
    "word2idx.to_csv(\"Data/word2idx.csv\", index=False)\n",
    "\n",
    "#validate the csv file by importing it\n",
    "word_idx_saved = pd.read_csv(\"Data/word2idx.csv\").to_dict('r')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting vectors for each word, since in the glove file each line is word followed by values for each dimension\n",
    "# of the embedding vector, separated by space\n",
    "embedding_dict = {}\n",
    "with open('./glove.6B.300d.txt','r') as f:\n",
    "  for line in f:\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    embedding = np.asarray(values[1:],'float32')\n",
    "    embedding_dict[word] = embedding\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of glove word embeddings: 400000\n",
      "Total word embeddings: 30000\n",
      "Null word embeddings: 12343\n"
     ]
    }
   ],
   "source": [
    "print('Number of glove word embeddings: %d' % len(embedding_dict))\n",
    "vocab_size = min(MAX_N_WORDS, len(word_idx)) # Only keep up to MAX_N_WORDS unique words\n",
    "word_embedding_matrix = np.zeros((vocab_size + 1, EMBEDDING_DIM)) # +1 reserved for padding\n",
    "for word, index in word_idx.items():\n",
    "  if index > MAX_N_WORDS: # word is not top MAX_N_WORDS frequent\n",
    "    continue\n",
    "  embedding = embedding_dict.get(word)\n",
    "  if embedding is not None:\n",
    "    word_embedding_matrix[index] = embedding\n",
    "print('Total word embeddings: %d' % (len(word_embedding_matrix) - 1))\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(word_embedding_matrix, axis=1) == 0))\n",
    "np.savetxt(\"Data/glove_word_embedding.csv\", word_embedding_matrix, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of train: (6248, 1000)\n",
      "Dimensions of test: (6248, 1000)\n",
      "Number of training labels (6248, 16)\n"
     ]
    }
   ],
   "source": [
    "# Pad the input so that it is valid input to the model\n",
    "\n",
    "train = pad_sequences(train_idx_seq, maxlen=MAX_SEQUENCE_LEN)\n",
    "\n",
    "test =  pad_sequences(test_idx_seq, maxlen=MAX_SEQUENCE_LEN)\n",
    "\n",
    "print('Dimensions of train:', train.shape)\n",
    "\n",
    "print('Dimensions of test:', test.shape)\n",
    "\n",
    "print('Number of training labels', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1000)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 1000, 300)         9000300   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                93440     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 128)               512       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9113820 (34.77 MB)\n",
      "Trainable params: 113072 (441.69 KB)\n",
      "Non-trainable params: 9000748 (34.34 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initializing Keras tensor\n",
    "train_tensor = Input(shape=(MAX_SEQUENCE_LEN,))\n",
    "\n",
    "test_tensor = Input(shape=(MAX_SEQUENCE_LEN,)) \n",
    "\n",
    "common_embed = Embedding(vocab_size + 1, # 0 is reserved for padding\n",
    "               EMBEDDING_DIM,\n",
    "               weights=[word_embedding_matrix],\n",
    "               input_length=MAX_SEQUENCE_LEN, # Maximum 50 words (index) in a input sequence\n",
    "               trainable=False)\n",
    "common_lstm = LSTM(LSTM_DIM)\n",
    "\n",
    "train_layer = common_embed(train_tensor)\n",
    "train_layer = common_lstm(train_layer)\n",
    "\n",
    "inputs = Dense(128, activation='relu')(train_layer)     #First\n",
    "inputs = Dropout(DROPOUT)(inputs)\n",
    "inputs = BatchNormalization()(inputs)\n",
    "inputs = Dense(64, activation='relu')(inputs)     #Second\n",
    "inputs = Dropout(DROPOUT)(inputs)\n",
    "inputs = BatchNormalization()(inputs)\n",
    "inputs = Dense(32, activation='relu')(inputs)     #Fourth\n",
    "inputs = Dropout(DROPOUT)(inputs)\n",
    "inputs = BatchNormalization()(inputs)\n",
    "\n",
    "final_output = Dense(16, activation='softmax')(inputs)\n",
    "\n",
    "model = Model(inputs=train_tensor, outputs=final_output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training at 2024-04-18 20:26:46.833097\n",
      "Epoch 1/10\n",
      "176/176 [==============================] - 64s 358ms/step - loss: 2.9525 - accuracy: 0.0866 - val_loss: 2.6283 - val_accuracy: 0.1376\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincent/Documents/Study/Software/anaconda3/envs/py3.8/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - 60s 339ms/step - loss: 2.6777 - accuracy: 0.1339 - val_loss: 2.5421 - val_accuracy: 0.1792\n",
      "Epoch 3/10\n",
      "176/176 [==============================] - 60s 339ms/step - loss: 2.5889 - accuracy: 0.1565 - val_loss: 2.5214 - val_accuracy: 0.1360\n",
      "Epoch 4/10\n",
      "176/176 [==============================] - 59s 334ms/step - loss: 2.5299 - accuracy: 0.1663 - val_loss: 2.5648 - val_accuracy: 0.1440\n",
      "Epoch 5/10\n",
      "176/176 [==============================] - 59s 336ms/step - loss: 2.4945 - accuracy: 0.1858 - val_loss: 2.5218 - val_accuracy: 0.1440\n",
      "Epoch 6/10\n",
      "176/176 [==============================] - 59s 334ms/step - loss: 2.4475 - accuracy: 0.1999 - val_loss: 2.5476 - val_accuracy: 0.1344\n",
      "Epoch 7/10\n",
      "176/176 [==============================] - 58s 330ms/step - loss: 2.4057 - accuracy: 0.2100 - val_loss: 2.5355 - val_accuracy: 0.1424\n",
      "Epoch 7: early stopping\n",
      "Training ended at 2024-04-18 20:33:45.228498\n",
      "Minutes elapsed: 6.973254\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training at\", datetime.datetime.now())\n",
    "t0 = time.time()\n",
    "callbacks = [ModelCheckpoint(\"baseline.h5\", monitor='val_accuracy', save_best_only=True),\n",
    "             EarlyStopping(monitor='val_accuracy', patience=5, verbose=1, mode='auto')]\n",
    "history = model.fit(train, \n",
    "                    y_train, \n",
    "                    epochs=10,\n",
    "                    validation_split=VALIDATION,\n",
    "                    verbose=1,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Training ended at\", datetime.datetime.now())\n",
    "print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"baseline.h5\")\n",
    "\n",
    "predicted = model.predict(test, verbose=0)\n",
    "predicted = np.argmax(predicted, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc =  0.15925096030729832\n"
     ]
    }
   ],
   "source": [
    "id2labels = {\n",
    "          0:'infp',\n",
    "          1:'infj',\n",
    "          2:'intp',\n",
    "          3:'intj',\n",
    "          4: 'istp',\n",
    "          5: 'istj',\n",
    "          6:'isfj',\n",
    "          7:'isfp',\n",
    "          8:'enfp',\n",
    "          9:'entp',\n",
    "          10:'enfj',\n",
    "          11:'entj',\n",
    "          12:'estp',\n",
    "          13:'estj',\n",
    "          14:'esfj',\n",
    "          15:'esfp'\n",
    "          }\n",
    "revised_predicted = [id2labels[int(i)] for i in predicted]\n",
    "acc = sum(revised_predicted == Y_test) / len(Y_test)\n",
    "print(\"Acc = \", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
